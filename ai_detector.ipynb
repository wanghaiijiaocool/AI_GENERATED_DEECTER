{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_data_path = \"./data/train_essays.csv\"\n",
    "train_prompts_path = \"./data/train_prompts.csv\"\n",
    "supplement_data_dir = \"./data/archive/\"\n",
    "supplement_data_files = [  os.path.join(supplement_data_dir,f)  \n",
    "                          for f in os.listdir(supplement_data_dir)\n",
    "                          if(f.endswith('.csv'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_proxy():\n",
    "    import os\n",
    "    cache_dir = \"/home/tx/workspace/cache\"  # 替换为你想要的缓存目录的路径\n",
    "    os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "    # 代理\n",
    "    os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "    os.environ['https_proxy'] = 'http://127.0.0.1:7890'\n",
    "    os.environ['no_proxy'] = '127.0.0.1,localhost'\n",
    "    os.environ['HTTP_PROXY'] = 'http://127.0.0.1:7890'\n",
    "    os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'\n",
    "    os.environ['NO_PROXY'] = '127.0.0.1,localhost'\n",
    "    return\n",
    "set_proxy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!curl huggingface.co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data=pd.read_csv(train_data_path)\n",
    "train_prompts = pd.read_csv(train_prompts_path)\n",
    "instructions = {\n",
    "    0:train_prompts['instructions'][0],\n",
    "    1:train_prompts['instructions'][1],\n",
    "}\n",
    "\n",
    "train_data['prompt'] = train_data.apply(\n",
    "    lambda r: instructions[r['prompt_id']] if  r['prompt_id'] in instructions else -1,axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0059830c</td>\n",
       "      <td>0</td>\n",
       "      <td>Cars. Cars have been around since they became ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>005db917</td>\n",
       "      <td>0</td>\n",
       "      <td>Transportation is a large necessity in most co...</td>\n",
       "      <td>0</td>\n",
       "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>008f63e3</td>\n",
       "      <td>0</td>\n",
       "      <td>\"America's love affair with it's vehicles seem...</td>\n",
       "      <td>0</td>\n",
       "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00940276</td>\n",
       "      <td>0</td>\n",
       "      <td>How often do you ride in a car? Do you drive a...</td>\n",
       "      <td>0</td>\n",
       "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00c39458</td>\n",
       "      <td>0</td>\n",
       "      <td>Cars are a wonderful thing. They are perhaps o...</td>\n",
       "      <td>0</td>\n",
       "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>fe6ff9a5</td>\n",
       "      <td>1</td>\n",
       "      <td>There has been a fuss about the Elector Colleg...</td>\n",
       "      <td>0</td>\n",
       "      <td>Write a letter to your state senator in which ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>ff669174</td>\n",
       "      <td>0</td>\n",
       "      <td>Limiting car usage has many advantages. Such a...</td>\n",
       "      <td>0</td>\n",
       "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>ffa247e0</td>\n",
       "      <td>0</td>\n",
       "      <td>There's a new trend that has been developing f...</td>\n",
       "      <td>0</td>\n",
       "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1376</th>\n",
       "      <td>ffc237e9</td>\n",
       "      <td>0</td>\n",
       "      <td>As we all know cars are a big part of our soci...</td>\n",
       "      <td>0</td>\n",
       "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>ffe1ca0d</td>\n",
       "      <td>0</td>\n",
       "      <td>Cars have been around since the 1800's and hav...</td>\n",
       "      <td>0</td>\n",
       "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1378 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  prompt_id                                               text  \\\n",
       "0     0059830c          0  Cars. Cars have been around since they became ...   \n",
       "1     005db917          0  Transportation is a large necessity in most co...   \n",
       "2     008f63e3          0  \"America's love affair with it's vehicles seem...   \n",
       "3     00940276          0  How often do you ride in a car? Do you drive a...   \n",
       "4     00c39458          0  Cars are a wonderful thing. They are perhaps o...   \n",
       "...        ...        ...                                                ...   \n",
       "1373  fe6ff9a5          1  There has been a fuss about the Elector Colleg...   \n",
       "1374  ff669174          0  Limiting car usage has many advantages. Such a...   \n",
       "1375  ffa247e0          0  There's a new trend that has been developing f...   \n",
       "1376  ffc237e9          0  As we all know cars are a big part of our soci...   \n",
       "1377  ffe1ca0d          0  Cars have been around since the 1800's and hav...   \n",
       "\n",
       "      generated                                             prompt  \n",
       "0             0  Write an explanatory essay to inform fellow ci...  \n",
       "1             0  Write an explanatory essay to inform fellow ci...  \n",
       "2             0  Write an explanatory essay to inform fellow ci...  \n",
       "3             0  Write an explanatory essay to inform fellow ci...  \n",
       "4             0  Write an explanatory essay to inform fellow ci...  \n",
       "...         ...                                                ...  \n",
       "1373          0  Write a letter to your state senator in which ...  \n",
       "1374          0  Write an explanatory essay to inform fellow ci...  \n",
       "1375          0  Write an explanatory essay to inform fellow ci...  \n",
       "1376          0  Write an explanatory essay to inform fellow ci...  \n",
       "1377          0  Write an explanatory essay to inform fellow ci...  \n",
       "\n",
       "[1378 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "supplement_train_data = pd.concat([\n",
    "    pd.read_csv(f)\n",
    "    for f in supplement_data_files\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    115372\n",
       "1     44084\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supplement_train_data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplement_train_data['generated'] = supplement_train_data['label']\n",
    "\n",
    "supplement_train_data = supplement_train_data[['text','generated','prompt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "#del train_data_re\n",
    "gc.collect()\n",
    "train_data_re = pd.DataFrame(train_data[['text','generated','prompt']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_all = pd.concat([train_data,supplement_train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generated\n",
       "0    116747\n",
       "1     44087\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_all.generated.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"/home/tx/workspace/cache\"  # 替换为你想要的缓存目录的路径\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "from transformers import AutoConfig, LlamaConfig \n",
    "# from llava.model.language_model.llava_llama import LlavaLlamaForCausalLM\n",
    "# class LlavaConfig(LlamaConfig):\n",
    "#     model_type = \"llava\"\n",
    "# AutoConfig.register(\"llava\", LlavaConfig)\n",
    "model_name = \"bigscience/bloom-3b\"\n",
    "#model_name = \"ChocoWu/nextgpt_7b_tiva_v0\"#\"liuhaotian/llava-v1.5-7b\"\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_name,cache_dir=cache_dir,trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,cache_dir=cache_dir)\n",
    "# tokenizer.add_tokens([\"[PAD]\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(prompt,feature_text):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    text=  f\"\"\"\n",
    "        read the following text, understand its style and words choices:\n",
    "        {feature_text}.\n",
    "        is this text generated by AI?\n",
    "    \"\"\"\n",
    "\n",
    "    # if(prompt is not None):\n",
    "    #     template_with_prompt = f\"\"\"\n",
    "    #     a prompt is shown as follows: \n",
    "    #     {prompt}.\n",
    "    #     text generated by this this prompt is shown as below:\n",
    "    #     {feature_text}.\n",
    "    #     is this text generated by AI?\n",
    "    #     \"\"\"\n",
    "    #     text = template_with_prompt\n",
    "    # else:\n",
    "    #     template_without_prompt = f\"\"\"\n",
    "    #     text generated without any prompt is shown as below:\n",
    "    #     {feature_text}.\n",
    "    #     is this text generated by AI?\n",
    "    #     \"\"\"\n",
    "    #     text = template_without_prompt\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import TensorDataset,DataLoader,RandomSampler\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class myDataset(TensorDataset):\n",
    "\n",
    "    def __init__(self, datalist,max_length=256,tokenizer=None,preprocess_func= None) -> None:\n",
    "        super(myDataset,self).__init__()\n",
    "\n",
    "        if(isinstance(datalist,pd.DataFrame)):\n",
    "            self.datalist = datalist.to_dict(orient='list')\n",
    "        elif(isinstance(datalist,dict)):\n",
    "            self.datalist = datalist\n",
    "        else:\n",
    "            raise Exception(\"错误输入类型\")\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.preprocess_func = preprocess_func\n",
    "    \n",
    "    def preprocess(self):\n",
    "        datalist_tmp = {\n",
    "            \"text\":[],\n",
    "            \"prompt\":[],\n",
    "            \"generated\":[]\n",
    "        }\n",
    "        for idx in tqdm(range(len(self))):\n",
    "           _, act_len,_ = self[idx]\n",
    "           if(act_len > self.max_length):\n",
    "               continue\n",
    "           \n",
    "           datalist_tmp['text'].append(self.datalist['text'][idx])\n",
    "           datalist_tmp['prompt'].append(self.datalist['prompt'][idx])\n",
    "           datalist_tmp['generated'].append(self.datalist['generated'][idx])\n",
    "        \n",
    "        self.datalist = datalist_tmp\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datalist['text'])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.datalist['text'][index]  \n",
    "        prompt = self.datalist['prompt'][index]  \n",
    "\n",
    "        final_text = self.preprocess_func(prompt,text)\n",
    "        input_ids = self.tokenizer.encode(final_text)\n",
    "        att_mask = [1] * len(input_ids)\n",
    "\n",
    "\n",
    "        labels = None\n",
    "        if('generated' in self.datalist):\n",
    "            generated = self.datalist['generated'][index]\n",
    "            label_text = \" the text is generated by AI\" if generated > 0 else \"the text is written by students\"\n",
    "            label_ids = self.tokenizer.encode(label_text)\n",
    "\n",
    "            labels = [self.tokenizer.pad_token_id]  * len(input_ids)\n",
    "            labels = labels + label_ids\n",
    "            input_ids = input_ids + label_ids   \n",
    "            att_mask = [1] * len(input_ids)\n",
    "        act_len = len(input_ids)\n",
    "        while(len(input_ids) < self.max_length):\n",
    "            input_ids.append(self.tokenizer.pad_token_id)\n",
    "            if(labels is not None):\n",
    "                labels.append(self.tokenizer.pad_token_id)\n",
    "            att_mask.append(0)\n",
    "        input_ids = input_ids[:self.max_length]\n",
    "        labels = labels[:self.max_length]\n",
    "        att_mask = att_mask[:self.max_length]\n",
    "        if(labels is not None):\n",
    "            return {'input_ids':torch.LongTensor(input_ids),'labels':torch.LongTensor(labels),'att_mask':torch.LongTensor(att_mask)},act_len,final_text\n",
    "        else:\n",
    "            return {'input_ids':torch.LongTensor(input_ids),'att_mask':torch.LongTensor(att_mask)}\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_all_sample = train_data_all.groupby(['generated']).sample(n=4000,replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed5b74cca514181b9c808b772e57ac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = myDataset(datalist=train_data_all_sample,max_length=512,tokenizer=tokenizer,preprocess_func=generate_prompt) \n",
    "\n",
    "dataset.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5969"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sampler = RandomSampler(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataloader_random = DataLoader(dataset, batch_size=2, sampler=random_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model(input_ids = torch.LongTensor([dataset[0]['input_ids']]),attention_mask=torch.LongTensor([dataset[0]['att_mask']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset[3][0]['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "For effortless bug reporting copy-paste your error into this form: https://docs.google.com/forms/d/e/1FAIpQLScPB8emS3Thkp66nvqwmjTEgxp8Y9ufuWTzFyr9kJ5AoI47dQ/viewform?usp=sf_link\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.7/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 5.2\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/tx/.conda/envs/llava/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tx/.conda/envs/llava/lib/python3.11/site-packages/bitsandbytes/cuda_setup/paths.py:93: UserWarning: /home/tx/.conda/envs/llava did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 163,840 || all params: 3,002,721,280 || trainable%: 0.005456383883888151\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from peft import LoraConfig, TaskType\n",
    "#lora_target_modules = [\"query_key_value\"]\n",
    "lora_target_modules = [ f\"transformer.h.{ly}.self_attention.query_key_value\" for ly in range(25,29) ]\n",
    "\n",
    "peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, \n",
    "                         inference_mode=False, target_modules=lora_target_modules,\n",
    "                         r=4, lora_alpha=16, lora_dropout=0.1)\n",
    "model = get_peft_model(original_model,peft_config)\n",
    "print(model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='mean',ignore_index=tokenizer.pad_token_id)\n",
    "def generate(prompt,model_,tokenizer_):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    input_ids = input_ids.to(model_.device)\n",
    "    #model_.disable_adapter()\n",
    "    # 生成文本\n",
    "    inputs = {\"input_ids\":input_ids, \"max_length\":512, \n",
    "              \"num_beams\":5, \"no_repeat_ngram_size\":2,\n",
    "             \"top_k\":50, \"top_p\":0.95}\n",
    "    with torch.no_grad():\n",
    "        output = model_.generate(**inputs)\n",
    "    # 将生成的token解码成文本\n",
    "    generated_text = tokenizer_.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        {'params': [p for p in model.parameters() if p.requires_grad],'lr': 5e-5},\n",
    "    ]\n",
    ")\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(dataloader_random) ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display,clear_output\n",
    "clear_output(wait=False)\n",
    "display(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        read the following text, understand its style and words choices:\n",
      "        This is true for me \"example is not the main thing in influencing others; it is the only thing.\". If you talk someone for one thing, information, film, music, game, ... Best support for your idea is examples. I think is the only thing for support your idea. You don't have other options But never mind you have a one more way for influence others everybody know this way because is the best way of everything it is the money.\n",
      "\n",
      "Once upon a time me and my friend talking about one film (i did not remember the film name know.) we are saying only examples (because we know is the best way for support our ideas) he is saying one example i am saying another and we are talking like a 30 or 35 min finally i gave up and i said if you accept my idea i will buy a large size pizza and he said OK.\n",
      "\n",
      "After for this conversation i learned example and money is not the main thing influencing others; it is the only thing\n",
      "\n",
      "..\n",
      "        is this text generated by AI?\n",
      "    ....\n",
      "\n",
      "A:\n",
      "\n",
      "I don't think this is an example of an AI-generated text. It sounds more like something written by a human.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/tx/workspace/code_for_compete/AI_GENERATED_DEECTER/ai_detector.ipynb 单元格 33\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B2.tcp.vip.cpolar.cn/home/tx/workspace/code_for_compete/AI_GENERATED_DEECTER/ai_detector.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m                 \u001b[39mprint\u001b[39m(test)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B2.tcp.vip.cpolar.cn/home/tx/workspace/code_for_compete/AI_GENERATED_DEECTER/ai_detector.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m                 pbar\u001b[39m.\u001b[39mdisplay()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B2.tcp.vip.cpolar.cn/home/tx/workspace/code_for_compete/AI_GENERATED_DEECTER/ai_detector.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m train()           \n",
      "\u001b[1;32m/home/tx/workspace/code_for_compete/AI_GENERATED_DEECTER/ai_detector.ipynb 单元格 33\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B2.tcp.vip.cpolar.cn/home/tx/workspace/code_for_compete/AI_GENERATED_DEECTER/ai_detector.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mif\u001b[39;00m((step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m2\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B2.tcp.vip.cpolar.cn/home/tx/workspace/code_for_compete/AI_GENERATED_DEECTER/ai_detector.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisplay\u001b[39;00m \u001b[39mimport\u001b[39;00m display,clear_output,update_display\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B2.tcp.vip.cpolar.cn/home/tx/workspace/code_for_compete/AI_GENERATED_DEECTER/ai_detector.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     test \u001b[39m=\u001b[39m generate(final_text[\u001b[39m0\u001b[39;49m],model,tokenizer)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B2.tcp.vip.cpolar.cn/home/tx/workspace/code_for_compete/AI_GENERATED_DEECTER/ai_detector.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     clear_output()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B2.tcp.vip.cpolar.cn/home/tx/workspace/code_for_compete/AI_GENERATED_DEECTER/ai_detector.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39mprint\u001b[39m(test)\n",
      "\u001b[1;32m/home/tx/workspace/code_for_compete/AI_GENERATED_DEECTER/ai_detector.ipynb 单元格 33\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B2.tcp.vip.cpolar.cn/home/tx/workspace/code_for_compete/AI_GENERATED_DEECTER/ai_detector.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m inputs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m:input_ids, \u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m512\u001b[39m, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B2.tcp.vip.cpolar.cn/home/tx/workspace/code_for_compete/AI_GENERATED_DEECTER/ai_detector.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnum_beams\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m5\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mno_repeat_ngram_size\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m2\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B2.tcp.vip.cpolar.cn/home/tx/workspace/code_for_compete/AI_GENERATED_DEECTER/ai_detector.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m          \u001b[39m\"\u001b[39m\u001b[39mtop_k\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m50\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtop_p\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m0.95\u001b[39m}\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B2.tcp.vip.cpolar.cn/home/tx/workspace/code_for_compete/AI_GENERATED_DEECTER/ai_detector.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B2.tcp.vip.cpolar.cn/home/tx/workspace/code_for_compete/AI_GENERATED_DEECTER/ai_detector.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     output \u001b[39m=\u001b[39m model_\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B2.tcp.vip.cpolar.cn/home/tx/workspace/code_for_compete/AI_GENERATED_DEECTER/ai_detector.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# 将生成的token解码成文本\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B2.tcp.vip.cpolar.cn/home/tx/workspace/code_for_compete/AI_GENERATED_DEECTER/ai_detector.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m generated_text \u001b[39m=\u001b[39m tokenizer_\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/llava/lib/python3.11/site-packages/peft/peft_model.py:975\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    973\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model\u001b[39m.\u001b[39mgeneration_config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeneration_config\n\u001b[1;32m    974\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 975\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    976\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model\u001b[39m.\u001b[39mprepare_inputs_for_generation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model_prepare_inputs_for_generation\n",
      "File \u001b[0;32m~/.conda/envs/llava/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/llava/lib/python3.11/site-packages/transformers/generation/utils.py:1685\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1678\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1679\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1680\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   1681\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1682\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1683\u001b[0m     )\n\u001b[1;32m   1684\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1685\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_search(\n\u001b[1;32m   1686\u001b[0m         input_ids,\n\u001b[1;32m   1687\u001b[0m         beam_scorer,\n\u001b[1;32m   1688\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1689\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1690\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1691\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1692\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1693\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1694\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1695\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1696\u001b[0m     )\n\u001b[1;32m   1698\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SAMPLE:\n\u001b[1;32m   1699\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m~/.conda/envs/llava/lib/python3.11/site-packages/transformers/generation/utils.py:3077\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m next_tokens \u001b[39m=\u001b[39m next_tokens \u001b[39m%\u001b[39m vocab_size\n\u001b[1;32m   3076\u001b[0m \u001b[39m# stateless\u001b[39;00m\n\u001b[0;32m-> 3077\u001b[0m beam_outputs \u001b[39m=\u001b[39m beam_scorer\u001b[39m.\u001b[39;49mprocess(\n\u001b[1;32m   3078\u001b[0m     input_ids,\n\u001b[1;32m   3079\u001b[0m     next_token_scores,\n\u001b[1;32m   3080\u001b[0m     next_tokens,\n\u001b[1;32m   3081\u001b[0m     next_indices,\n\u001b[1;32m   3082\u001b[0m     pad_token_id\u001b[39m=\u001b[39;49mpad_token_id,\n\u001b[1;32m   3083\u001b[0m     eos_token_id\u001b[39m=\u001b[39;49meos_token_id,\n\u001b[1;32m   3084\u001b[0m     beam_indices\u001b[39m=\u001b[39;49mbeam_indices,\n\u001b[1;32m   3085\u001b[0m )\n\u001b[1;32m   3087\u001b[0m beam_scores \u001b[39m=\u001b[39m beam_outputs[\u001b[39m\"\u001b[39m\u001b[39mnext_beam_scores\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   3088\u001b[0m beam_next_tokens \u001b[39m=\u001b[39m beam_outputs[\u001b[39m\"\u001b[39m\u001b[39mnext_beam_tokens\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/llava/lib/python3.11/site-packages/transformers/generation/beam_search.py:269\u001b[0m, in \u001b[0;36mBeamSearchScorer.process\u001b[0;34m(self, input_ids, next_scores, next_tokens, next_indices, pad_token_id, eos_token_id, beam_indices, group_index)\u001b[0m\n\u001b[1;32m    267\u001b[0m batch_beam_idx \u001b[39m=\u001b[39m batch_idx \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroup_size \u001b[39m+\u001b[39m next_index\n\u001b[1;32m    268\u001b[0m \u001b[39m# add to generated hypotheses if end of sentence\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m \u001b[39mif\u001b[39;00m (eos_token_id \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (next_token\u001b[39m.\u001b[39;49mitem() \u001b[39min\u001b[39;00m eos_token_id):\n\u001b[1;32m    270\u001b[0m     \u001b[39m# if beam_token does not belong to top num_beams tokens, it should not be added\u001b[39;00m\n\u001b[1;32m    271\u001b[0m     is_beam_token_worse_than_top_num_beams \u001b[39m=\u001b[39m beam_token_rank \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroup_size\n\u001b[1;32m    272\u001b[0m     \u001b[39mif\u001b[39;00m is_beam_token_worse_than_top_num_beams:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    num_epochs = 1\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        pbar = tqdm(dataloader_random)\n",
    "        for step, batch_a in enumerate(pbar):\n",
    "            batch,_,final_text = batch_a\n",
    "            batch = {k: v.cuda() for k, v in batch.items()}\n",
    "            #print(batch)\n",
    "            #outputs = model(batch['input_ids'],labels=batch['labels'])\n",
    "            labels_tensor = batch['labels']\n",
    "            outputs = model(batch['input_ids'])\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            logits = logits[...,:-1,:].contiguous()\n",
    "            labels_tensor = labels_tensor[...,1:].contiguous()\n",
    "            \n",
    "            \n",
    "            #loss = outputs.loss\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels_tensor.view(-1))\n",
    "        \n",
    "            total_loss += loss.detach().float()\n",
    "            #print(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            if(step % 10 == 0):\n",
    "                pbar.set_description(f\"step {step} loss {loss}\")\n",
    "                print(loss)\n",
    "            if(step  % 200 == 0 and step > 0):\n",
    "                from IPython.display import display,clear_output,update_display\n",
    "                test = generate(final_text[0],model,tokenizer)\n",
    "                print(test)\n",
    "                \n",
    "\n",
    "                \n",
    "            \n",
    "train()           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "import time\n",
    "clear_output()\n",
    "\n",
    "out = widgets.Output()\n",
    "\n",
    "for fruit in [1,2,3]:\n",
    "    time.sleep(1)\n",
    "    clear_output()\n",
    "    #out.append_stdout(f\"Do you like {fruit}s?\")\n",
    "    print(fruit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_trained = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tx/.conda/envs/llava/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:368: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    read the following text, understand its style and words choices:\\n    maybe its is not rigiht what.\\n    is this text generated by AI?\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"\"\"\n",
    "    read the following text, understand its style and words choices:\n",
    "    maybe its is not rigiht what.\n",
    "    is this text generated by AI?\n",
    "\"\"\",model_trained,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data):\n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = \"./data/test_essays.csv\"\n",
    "test_data = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000aaaa</td>\n",
       "      <td>2</td>\n",
       "      <td>Aaa bbb ccc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1111bbbb</td>\n",
       "      <td>3</td>\n",
       "      <td>Bbb ccc ddd.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2222cccc</td>\n",
       "      <td>4</td>\n",
       "      <td>CCC ddd eee.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  prompt_id          text\n",
       "0  0000aaaa          2  Aaa bbb ccc.\n",
       "1  1111bbbb          3  Bbb ccc ddd.\n",
       "2  2222cccc          4  CCC ddd eee."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "save_dir =  \"/home/tx/workspace/saved_model\"\n",
    "if(not os.path.exists(save_dir)):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_name = os.path.join(save_dir,'bloom_3b_AI_detector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(model_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
