{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_data_path = \"./data/train_essays.csv\"\n",
    "train_prompts_path = \"./data/train_prompts.csv\"\n",
    "supplement_data_dir = \"./data/archive/\"\n",
    "supplement_data_files = [  os.path.join(supplement_data_dir,f)  \n",
    "                          for f in os.listdir(supplement_data_dir)\n",
    "                          if(f.endswith('.csv'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_proxy():\n",
    "    import os\n",
    "    cache_dir = \"/home/tx/workspace/cache\"  # 替换为你想要的缓存目录的路径\n",
    "    os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "    # 代理\n",
    "    os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "    os.environ['https_proxy'] = 'http://127.0.0.1:7890'\n",
    "    os.environ['no_proxy'] = '127.0.0.1,localhost'\n",
    "    os.environ['HTTP_PROXY'] = 'http://127.0.0.1:7890'\n",
    "    os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'\n",
    "    os.environ['NO_PROXY'] = '127.0.0.1,localhost'\n",
    "    return\n",
    "set_proxy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!curl huggingface.co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data=pd.read_csv(train_data_path)\n",
    "train_prompts = pd.read_csv(train_prompts_path)\n",
    "instructions = {\n",
    "    0:train_prompts['instructions'][0],\n",
    "    1:train_prompts['instructions'][1],\n",
    "}\n",
    "\n",
    "train_data['prompt'] = train_data.apply(\n",
    "    lambda r: instructions[r['prompt_id']] if  r['prompt_id'] in instructions else -1,axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0059830c</td>\n",
       "      <td>0</td>\n",
       "      <td>Cars. Cars have been around since they became ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>005db917</td>\n",
       "      <td>0</td>\n",
       "      <td>Transportation is a large necessity in most co...</td>\n",
       "      <td>0</td>\n",
       "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>008f63e3</td>\n",
       "      <td>0</td>\n",
       "      <td>\"America's love affair with it's vehicles seem...</td>\n",
       "      <td>0</td>\n",
       "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00940276</td>\n",
       "      <td>0</td>\n",
       "      <td>How often do you ride in a car? Do you drive a...</td>\n",
       "      <td>0</td>\n",
       "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00c39458</td>\n",
       "      <td>0</td>\n",
       "      <td>Cars are a wonderful thing. They are perhaps o...</td>\n",
       "      <td>0</td>\n",
       "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>fe6ff9a5</td>\n",
       "      <td>1</td>\n",
       "      <td>There has been a fuss about the Elector Colleg...</td>\n",
       "      <td>0</td>\n",
       "      <td>Write a letter to your state senator in which ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>ff669174</td>\n",
       "      <td>0</td>\n",
       "      <td>Limiting car usage has many advantages. Such a...</td>\n",
       "      <td>0</td>\n",
       "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>ffa247e0</td>\n",
       "      <td>0</td>\n",
       "      <td>There's a new trend that has been developing f...</td>\n",
       "      <td>0</td>\n",
       "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1376</th>\n",
       "      <td>ffc237e9</td>\n",
       "      <td>0</td>\n",
       "      <td>As we all know cars are a big part of our soci...</td>\n",
       "      <td>0</td>\n",
       "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>ffe1ca0d</td>\n",
       "      <td>0</td>\n",
       "      <td>Cars have been around since the 1800's and hav...</td>\n",
       "      <td>0</td>\n",
       "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1378 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  prompt_id                                               text  \\\n",
       "0     0059830c          0  Cars. Cars have been around since they became ...   \n",
       "1     005db917          0  Transportation is a large necessity in most co...   \n",
       "2     008f63e3          0  \"America's love affair with it's vehicles seem...   \n",
       "3     00940276          0  How often do you ride in a car? Do you drive a...   \n",
       "4     00c39458          0  Cars are a wonderful thing. They are perhaps o...   \n",
       "...        ...        ...                                                ...   \n",
       "1373  fe6ff9a5          1  There has been a fuss about the Elector Colleg...   \n",
       "1374  ff669174          0  Limiting car usage has many advantages. Such a...   \n",
       "1375  ffa247e0          0  There's a new trend that has been developing f...   \n",
       "1376  ffc237e9          0  As we all know cars are a big part of our soci...   \n",
       "1377  ffe1ca0d          0  Cars have been around since the 1800's and hav...   \n",
       "\n",
       "      generated                                             prompt  \n",
       "0             0  Write an explanatory essay to inform fellow ci...  \n",
       "1             0  Write an explanatory essay to inform fellow ci...  \n",
       "2             0  Write an explanatory essay to inform fellow ci...  \n",
       "3             0  Write an explanatory essay to inform fellow ci...  \n",
       "4             0  Write an explanatory essay to inform fellow ci...  \n",
       "...         ...                                                ...  \n",
       "1373          0  Write a letter to your state senator in which ...  \n",
       "1374          0  Write an explanatory essay to inform fellow ci...  \n",
       "1375          0  Write an explanatory essay to inform fellow ci...  \n",
       "1376          0  Write an explanatory essay to inform fellow ci...  \n",
       "1377          0  Write an explanatory essay to inform fellow ci...  \n",
       "\n",
       "[1378 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "supplement_train_data = pd.concat([\n",
    "    pd.read_csv(f)\n",
    "    for f in supplement_data_files\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    115372\n",
       "1     44084\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supplement_train_data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplement_train_data['generated'] = supplement_train_data['label']\n",
    "\n",
    "supplement_train_data = supplement_train_data[['text','generated','prompt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "#del train_data_re\n",
    "gc.collect()\n",
    "train_data_re = pd.DataFrame(train_data[['text','generated','prompt']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_all = pd.concat([train_data,supplement_train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generated\n",
       "0    116747\n",
       "1     44087\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_all.generated.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"/home/tx/workspace/cache\"  # 替换为你想要的缓存目录的路径\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "from transformers import AutoConfig, LlamaConfig \n",
    "# from llava.model.language_model.llava_llama import LlavaLlamaForCausalLM\n",
    "# class LlavaConfig(LlamaConfig):\n",
    "#     model_type = \"llava\"\n",
    "# AutoConfig.register(\"llava\", LlavaConfig)\n",
    "model_name = \"bigscience/bloom-3b\"\n",
    "#model_name = \"ChocoWu/nextgpt_7b_tiva_v0\"#\"liuhaotian/llava-v1.5-7b\"\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_name,cache_dir=cache_dir,trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,cache_dir=cache_dir)\n",
    "# tokenizer.add_tokens([\"[PAD]\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(prompt,feature_text):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    text=  f\"\"\"\n",
    "        read the following text, understand its style and words choices:\n",
    "        {feature_text}.\n",
    "        is this text generated by AI?\n",
    "    \"\"\"\n",
    "\n",
    "    # if(prompt is not None):\n",
    "    #     template_with_prompt = f\"\"\"\n",
    "    #     a prompt is shown as follows: \n",
    "    #     {prompt}.\n",
    "    #     text generated by this this prompt is shown as below:\n",
    "    #     {feature_text}.\n",
    "    #     is this text generated by AI?\n",
    "    #     \"\"\"\n",
    "    #     text = template_with_prompt\n",
    "    # else:\n",
    "    #     template_without_prompt = f\"\"\"\n",
    "    #     text generated without any prompt is shown as below:\n",
    "    #     {feature_text}.\n",
    "    #     is this text generated by AI?\n",
    "    #     \"\"\"\n",
    "    #     text = template_without_prompt\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import TensorDataset,DataLoader,RandomSampler\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class myDataset(TensorDataset):\n",
    "\n",
    "    def __init__(self, datalist,max_length=256,tokenizer=None,preprocess_func= None) -> None:\n",
    "        super(myDataset,self).__init__()\n",
    "\n",
    "        if(isinstance(datalist,pd.DataFrame)):\n",
    "            self.datalist = datalist.to_dict(orient='list')\n",
    "        elif(isinstance(datalist,dict)):\n",
    "            self.datalist = datalist\n",
    "        else:\n",
    "            raise Exception(\"错误输入类型\")\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.preprocess_func = preprocess_func\n",
    "    \n",
    "    def preprocess(self):\n",
    "        datalist_tmp = {\n",
    "            \"text\":[],\n",
    "            \"prompt\":[],\n",
    "            \"generated\":[]\n",
    "        }\n",
    "        for idx in tqdm(range(len(self))):\n",
    "           _, act_len,_,_ = self[idx]\n",
    "           if(act_len > self.max_length):\n",
    "               continue\n",
    "           \n",
    "           datalist_tmp['text'].append(self.datalist['text'][idx])\n",
    "           datalist_tmp['prompt'].append(self.datalist['prompt'][idx])\n",
    "           datalist_tmp['generated'].append(self.datalist['generated'][idx])\n",
    "        \n",
    "        self.datalist = datalist_tmp\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datalist['text'])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.datalist['text'][index]  \n",
    "        prompt = self.datalist['prompt'][index]  \n",
    "\n",
    "        final_text = self.preprocess_func(prompt,text)\n",
    "        input_ids = self.tokenizer.encode(final_text)\n",
    "        att_mask = [1] * len(input_ids)\n",
    "\n",
    "\n",
    "        labels = None\n",
    "        if('generated' in self.datalist):\n",
    "            generated = self.datalist['generated'][index]\n",
    "            label_text = \"yes, the text is generated by AI\" if generated > 0 else \"no, the text is written by students\"\n",
    "            label_ids = self.tokenizer.encode(label_text)\n",
    "\n",
    "            #final_text = [final_text,label_text]\n",
    "\n",
    "            labels = [self.tokenizer.pad_token_id]  * len(input_ids)\n",
    "            labels = labels + label_ids + [self.tokenizer.eos_token_id]\n",
    "            input_ids = input_ids + label_ids   \n",
    "            att_mask = [1] * len(input_ids)\n",
    "        act_len = len(input_ids)\n",
    "        while(len(input_ids) < self.max_length):\n",
    "            input_ids.append(self.tokenizer.pad_token_id)\n",
    "            if(labels is not None):\n",
    "                labels.append(self.tokenizer.pad_token_id)\n",
    "            att_mask.append(0)\n",
    "        input_ids = input_ids[:self.max_length]\n",
    "        labels = labels[:self.max_length]\n",
    "        att_mask = att_mask[:self.max_length]\n",
    "        if(labels is not None):\n",
    "            return {'input_ids':torch.LongTensor(input_ids),'labels':torch.LongTensor(labels),'att_mask':torch.LongTensor(att_mask)},act_len,final_text,label_text\n",
    "        else:\n",
    "            return {'input_ids':torch.LongTensor(input_ids),'att_mask':torch.LongTensor(att_mask)}\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_all_sample = train_data_all.groupby(['generated']).sample(n=4000,replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b5081f557843e9999f33458487f998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = myDataset(datalist=train_data_all_sample,max_length=512,tokenizer=tokenizer,preprocess_func=generate_prompt) \n",
    "\n",
    "dataset.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5993"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sampler = RandomSampler(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataloader_random = DataLoader(dataset, batch_size=2, sampler=random_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "For effortless bug reporting copy-paste your error into this form: https://docs.google.com/forms/d/e/1FAIpQLScPB8emS3Thkp66nvqwmjTEgxp8Y9ufuWTzFyr9kJ5AoI47dQ/viewform?usp=sf_link\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.7/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 5.2\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/tx/.conda/envs/llava/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tx/.conda/envs/llava/lib/python3.11/site-packages/bitsandbytes/cuda_setup/paths.py:93: UserWarning: /home/tx/.conda/envs/llava did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 163,840 || all params: 3,002,721,280 || trainable%: 0.005456383883888151\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from peft import LoraConfig, TaskType\n",
    "#lora_target_modules = [\"query_key_value\"]\n",
    "lora_target_modules = [ f\"transformer.h.{ly}.self_attention.query_key_value\" for ly in range(25,29) ]\n",
    "\n",
    "peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, \n",
    "                         inference_mode=False, target_modules=lora_target_modules,\n",
    "                         r=4, lora_alpha=16, lora_dropout=0.1)\n",
    "model = get_peft_model(original_model,peft_config)\n",
    "print(model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='mean',ignore_index=tokenizer.pad_token_id)\n",
    "def generate(prompt,model_,tokenizer_):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    input_ids = input_ids.to(model_.device)\n",
    "    #model_.disable_adapter()\n",
    "    # 生成文本\n",
    "    inputs = {\"input_ids\":input_ids, \"max_length\":512, \n",
    "              \"num_beams\":5, \"no_repeat_ngram_size\":2,\n",
    "             \"top_k\":50, \"top_p\":0.95}\n",
    "    with torch.no_grad():\n",
    "        output = model_.generate(**inputs)\n",
    "    # 将生成的token解码成文本\n",
    "    generated_text = tokenizer_.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        {'params': [p for p in model.parameters() if p.requires_grad],'lr': 5e-5},\n",
    "    ]\n",
    ")\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(dataloader_random) ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f99738d75ed4dcc8f34bd40aa6be667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2997 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.6698, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6164, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tx/.conda/envs/llava/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:368: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      " the text is generated by AI\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "        read the following text, understand its style and words choices:\n",
      "         I would not support the idea of adding an extra hour and a half to the school day. While I understand that some students may want to participate in additional activities or stay after school to complete assignments and projects, I believe that the current seven-hour school day is already sufficient.\n",
      "\n",
      "I believe that parents may be concerned about their children's safety and well-being if they are away from home for an additional hour or two. Additionally, I think that students should have the opportunity to spend time with their families and engage in extracurricular activities outside of school.\n",
      "\n",
      "In my opinion, it is not fair for the students to decide if they want an extra hour or half an hour of school. The decision should be made by the parents and teachers, who are responsible for the students' education and well-being.\n",
      "\n",
      "I understand that students may have a lot of work to do at school, but I believe that it is important to maintain a balance between academic and personal responsibilities. Students should be encouraged to prioritize their studies, but they should also be given the opportunity to take breaks and engage in activities that bring them joy and fulfillment.\n",
      "\n",
      "In conclusion, while I understand the desire for an extra hour of school, I believe that the current seven-hour school day is sufficient. It is important to consider the safety and well-being of students, as well as the importance of maintaining a balance between academic and personal responsibilities..\n",
      "        is this text generated by AI?\n",
      "     the text is written by students\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    num_epochs = 1\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        pbar = tqdm(dataloader_random)\n",
    "        for step, batch_a in enumerate(pbar):\n",
    "            batch,_,final_text,label_text = batch_a\n",
    "            batch = {k: v.cuda() for k, v in batch.items()}\n",
    "            #print(batch)\n",
    "            #outputs = model(batch['input_ids'],labels=batch['labels'])\n",
    "            labels_tensor = batch['labels']\n",
    "            outputs = model(batch['input_ids'])\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            logits = logits[...,:-1,:].contiguous()\n",
    "            labels_tensor = labels_tensor[...,1:].contiguous()\n",
    "            \n",
    "            \n",
    "            #loss = outputs.loss\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels_tensor.view(-1))\n",
    "        \n",
    "            total_loss += loss.detach().float()\n",
    "            #print(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            if(step % 100 == 0):\n",
    "                pbar.set_description(f\"step {step} loss {loss}\")\n",
    "                print(loss)\n",
    "            if(step  % 200 == 0 and step > 0):\n",
    "                from IPython.display import display,clear_output,update_display\n",
    "                test = generate(final_text[0],model,tokenizer)\n",
    "                print(\"=\"* 100)\n",
    "                print(label_text[0])\n",
    "                print(\">\"* 100)\n",
    "                print(test)\n",
    "                \n",
    "\n",
    "                \n",
    "            \n",
    "train()           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "import time\n",
    "clear_output()\n",
    "\n",
    "out = widgets.Output()\n",
    "\n",
    "for fruit in [1,2,3]:\n",
    "    time.sleep(1)\n",
    "    clear_output()\n",
    "    #out.append_stdout(f\"Do you like {fruit}s?\")\n",
    "    print(fruit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_trained = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tx/.conda/envs/llava/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:368: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    read the following text, understand its style and words choices:\\n    maybe its is not rigiht what.\\n    is this text generated by AI?\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"\"\"\n",
    "    read the following text, understand its style and words choices:\n",
    "    maybe its is not rigiht what.\n",
    "    is this text generated by AI?\n",
    "\"\"\",model_trained,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data):\n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = \"./data/test_essays.csv\"\n",
    "test_data = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000aaaa</td>\n",
       "      <td>2</td>\n",
       "      <td>Aaa bbb ccc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1111bbbb</td>\n",
       "      <td>3</td>\n",
       "      <td>Bbb ccc ddd.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2222cccc</td>\n",
       "      <td>4</td>\n",
       "      <td>CCC ddd eee.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  prompt_id          text\n",
       "0  0000aaaa          2  Aaa bbb ccc.\n",
       "1  1111bbbb          3  Bbb ccc ddd.\n",
       "2  2222cccc          4  CCC ddd eee."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "save_dir =  \"/home/tx/workspace/saved_model\"\n",
    "if(not os.path.exists(save_dir)):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_name = os.path.join(save_dir,'bloom_3b_AI_detector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(model_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
