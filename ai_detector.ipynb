{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_data_path = \"./data/train_essays.csv\"\n",
    "train_prompts_path = \"./data/train_prompts.csv\"\n",
    "supplement_data_dir = \"./data/archive/\"\n",
    "supplement_data_files = [  os.path.join(supplement_data_dir,f)  \n",
    "                          for f in os.listdir(supplement_data_dir)\n",
    "                          if(f.endswith('.csv'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_proxy():\n",
    "    import os\n",
    "    cache_dir = \"/home/tx/workspace/cache\"  # 替换为你想要的缓存目录的路径\n",
    "    os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "    # 代理\n",
    "    os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "    os.environ['https_proxy'] = 'http://127.0.0.1:7890'\n",
    "    os.environ['no_proxy'] = '127.0.0.1,localhost'\n",
    "    os.environ['HTTP_PROXY'] = 'http://127.0.0.1:7890'\n",
    "    os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'\n",
    "    os.environ['NO_PROXY'] = '127.0.0.1,localhost'\n",
    "    return\n",
    "set_proxy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!curl huggingface.co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data=pd.read_csv(train_data_path)\n",
    "train_prompts = pd.read_csv(train_prompts_path)\n",
    "instructions = {\n",
    "    0:train_prompts['instructions'][0],\n",
    "    1:train_prompts['instructions'][1],\n",
    "}\n",
    "\n",
    "train_data['prompt'] = train_data.apply(\n",
    "    lambda r: instructions[r['prompt_id']] if  r['prompt_id'] in instructions else -1,axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0059830c</td>\n",
       "      <td>0</td>\n",
       "      <td>Cars. Cars have been around since they became ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>005db917</td>\n",
       "      <td>0</td>\n",
       "      <td>Transportation is a large necessity in most co...</td>\n",
       "      <td>0</td>\n",
       "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>008f63e3</td>\n",
       "      <td>0</td>\n",
       "      <td>\"America's love affair with it's vehicles seem...</td>\n",
       "      <td>0</td>\n",
       "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00940276</td>\n",
       "      <td>0</td>\n",
       "      <td>How often do you ride in a car? Do you drive a...</td>\n",
       "      <td>0</td>\n",
       "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00c39458</td>\n",
       "      <td>0</td>\n",
       "      <td>Cars are a wonderful thing. They are perhaps o...</td>\n",
       "      <td>0</td>\n",
       "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>fe6ff9a5</td>\n",
       "      <td>1</td>\n",
       "      <td>There has been a fuss about the Elector Colleg...</td>\n",
       "      <td>0</td>\n",
       "      <td>Write a letter to your state senator in which ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>ff669174</td>\n",
       "      <td>0</td>\n",
       "      <td>Limiting car usage has many advantages. Such a...</td>\n",
       "      <td>0</td>\n",
       "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>ffa247e0</td>\n",
       "      <td>0</td>\n",
       "      <td>There's a new trend that has been developing f...</td>\n",
       "      <td>0</td>\n",
       "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1376</th>\n",
       "      <td>ffc237e9</td>\n",
       "      <td>0</td>\n",
       "      <td>As we all know cars are a big part of our soci...</td>\n",
       "      <td>0</td>\n",
       "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>ffe1ca0d</td>\n",
       "      <td>0</td>\n",
       "      <td>Cars have been around since the 1800's and hav...</td>\n",
       "      <td>0</td>\n",
       "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1378 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  prompt_id                                               text  \\\n",
       "0     0059830c          0  Cars. Cars have been around since they became ...   \n",
       "1     005db917          0  Transportation is a large necessity in most co...   \n",
       "2     008f63e3          0  \"America's love affair with it's vehicles seem...   \n",
       "3     00940276          0  How often do you ride in a car? Do you drive a...   \n",
       "4     00c39458          0  Cars are a wonderful thing. They are perhaps o...   \n",
       "...        ...        ...                                                ...   \n",
       "1373  fe6ff9a5          1  There has been a fuss about the Elector Colleg...   \n",
       "1374  ff669174          0  Limiting car usage has many advantages. Such a...   \n",
       "1375  ffa247e0          0  There's a new trend that has been developing f...   \n",
       "1376  ffc237e9          0  As we all know cars are a big part of our soci...   \n",
       "1377  ffe1ca0d          0  Cars have been around since the 1800's and hav...   \n",
       "\n",
       "      generated                                             prompt  \n",
       "0             0  Write an explanatory essay to inform fellow ci...  \n",
       "1             0  Write an explanatory essay to inform fellow ci...  \n",
       "2             0  Write an explanatory essay to inform fellow ci...  \n",
       "3             0  Write an explanatory essay to inform fellow ci...  \n",
       "4             0  Write an explanatory essay to inform fellow ci...  \n",
       "...         ...                                                ...  \n",
       "1373          0  Write a letter to your state senator in which ...  \n",
       "1374          0  Write an explanatory essay to inform fellow ci...  \n",
       "1375          0  Write an explanatory essay to inform fellow ci...  \n",
       "1376          0  Write an explanatory essay to inform fellow ci...  \n",
       "1377          0  Write an explanatory essay to inform fellow ci...  \n",
       "\n",
       "[1378 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "supplement_train_data = pd.concat([\n",
    "    pd.read_csv(f)\n",
    "    for f in supplement_data_files\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    115372\n",
       "1     44084\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supplement_train_data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplement_train_data['generated'] = supplement_train_data['label']\n",
    "\n",
    "supplement_train_data = supplement_train_data[['text','generated','prompt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "#del train_data_re\n",
    "gc.collect()\n",
    "train_data_re = pd.DataFrame(train_data[['text','generated','prompt']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_all = pd.concat([train_data,supplement_train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generated\n",
       "0    116747\n",
       "1     44087\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_all.generated.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"/home/tx/workspace/cache\"  # 替换为你想要的缓存目录的路径\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "from transformers import AutoConfig, LlamaConfig \n",
    "# from llava.model.language_model.llava_llama import LlavaLlamaForCausalLM\n",
    "# class LlavaConfig(LlamaConfig):\n",
    "#     model_type = \"llava\"\n",
    "# AutoConfig.register(\"llava\", LlavaConfig)\n",
    "model_name = \"bigscience/bloom-3b\"\n",
    "#model_name = \"ChocoWu/nextgpt_7b_tiva_v0\"#\"liuhaotian/llava-v1.5-7b\"\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_name,cache_dir=cache_dir,trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,cache_dir=cache_dir)\n",
    "# tokenizer.add_tokens([\"[PAD]\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(prompt,feature_text):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    text=  f\"\"\"\n",
    "        read the following text, understand its style and words choices:\n",
    "        {feature_text}.\n",
    "        is this text generated by AI?\n",
    "    \"\"\"\n",
    "\n",
    "    # if(prompt is not None):\n",
    "    #     template_with_prompt = f\"\"\"\n",
    "    #     a prompt is shown as follows: \n",
    "    #     {prompt}.\n",
    "    #     text generated by this this prompt is shown as below:\n",
    "    #     {feature_text}.\n",
    "    #     is this text generated by AI?\n",
    "    #     \"\"\"\n",
    "    #     text = template_with_prompt\n",
    "    # else:\n",
    "    #     template_without_prompt = f\"\"\"\n",
    "    #     text generated without any prompt is shown as below:\n",
    "    #     {feature_text}.\n",
    "    #     is this text generated by AI?\n",
    "    #     \"\"\"\n",
    "    #     text = template_without_prompt\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import TensorDataset,DataLoader,RandomSampler\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class myDataset(TensorDataset):\n",
    "\n",
    "    def __init__(self, datalist,max_length=256,tokenizer=None,preprocess_func= None) -> None:\n",
    "        super(myDataset,self).__init__()\n",
    "\n",
    "        if(isinstance(datalist,pd.DataFrame)):\n",
    "            self.datalist = datalist.to_dict(orient='list')\n",
    "        elif(isinstance(datalist,dict)):\n",
    "            self.datalist = datalist\n",
    "        else:\n",
    "            raise Exception(\"错误输入类型\")\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.preprocess_func = preprocess_func\n",
    "    \n",
    "    def preprocess(self):\n",
    "        datalist_tmp = {\n",
    "            \"text\":[],\n",
    "            \"prompt\":[],\n",
    "            \"generated\":[]\n",
    "        }\n",
    "        for idx in tqdm(range(len(self))):\n",
    "           _, act_len,_,_ = self[idx]\n",
    "           if(act_len > self.max_length):\n",
    "               continue\n",
    "           \n",
    "           datalist_tmp['text'].append(self.datalist['text'][idx])\n",
    "           datalist_tmp['prompt'].append(self.datalist['prompt'][idx])\n",
    "           datalist_tmp['generated'].append(self.datalist['generated'][idx])\n",
    "        \n",
    "        self.datalist = datalist_tmp\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datalist['text'])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.datalist['text'][index]  \n",
    "        prompt = self.datalist['prompt'][index]  \n",
    "\n",
    "        final_text = self.preprocess_func(prompt,text)\n",
    "        input_ids = self.tokenizer.encode(final_text)\n",
    "        att_mask = [1] * len(input_ids)\n",
    "\n",
    "\n",
    "        labels = None\n",
    "        if('generated' in self.datalist):\n",
    "            generated = self.datalist['generated'][index]\n",
    "            label_text = \"yes, the text is generated\" if generated > 0 else \"no, the text is written by students\"\n",
    "            label_ids = self.tokenizer.encode(label_text)\n",
    "\n",
    "            #final_text = [final_text,label_text]\n",
    "\n",
    "            labels = [self.tokenizer.pad_token_id]  * len(input_ids)\n",
    "            labels = labels + label_ids + [self.tokenizer.eos_token_id]\n",
    "            input_ids = input_ids + label_ids   \n",
    "            att_mask = [1] * len(input_ids)\n",
    "        act_len = len(input_ids)\n",
    "        while(len(input_ids) < self.max_length):\n",
    "            input_ids.append(self.tokenizer.pad_token_id)\n",
    "            if(labels is not None):\n",
    "                labels.append(self.tokenizer.pad_token_id)\n",
    "            att_mask.append(0)\n",
    "        input_ids = input_ids[:self.max_length]\n",
    "        labels = labels[:self.max_length]\n",
    "        att_mask = att_mask[:self.max_length]\n",
    "        if(labels is not None):\n",
    "            return {'input_ids':torch.LongTensor(input_ids),'labels':torch.LongTensor(labels),'att_mask':torch.LongTensor(att_mask)},act_len,final_text,label_text\n",
    "        else:\n",
    "            return {'input_ids':torch.LongTensor(input_ids),'att_mask':torch.LongTensor(att_mask)}\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_all_sample = train_data_all.groupby(['generated']).sample(n=4000,replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b5081f557843e9999f33458487f998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = myDataset(datalist=train_data_all_sample,max_length=512,tokenizer=tokenizer,preprocess_func=generate_prompt) \n",
    "\n",
    "dataset.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5993"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sampler = RandomSampler(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataloader_random = DataLoader(dataset, batch_size=2, sampler=random_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "For effortless bug reporting copy-paste your error into this form: https://docs.google.com/forms/d/e/1FAIpQLScPB8emS3Thkp66nvqwmjTEgxp8Y9ufuWTzFyr9kJ5AoI47dQ/viewform?usp=sf_link\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.7/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 5.2\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/tx/.conda/envs/llava/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tx/.conda/envs/llava/lib/python3.11/site-packages/bitsandbytes/cuda_setup/paths.py:93: UserWarning: /home/tx/.conda/envs/llava did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 163,840 || all params: 3,002,721,280 || trainable%: 0.005456383883888151\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from peft import LoraConfig, TaskType\n",
    "#lora_target_modules = [\"query_key_value\"]\n",
    "lora_target_modules = [ f\"transformer.h.{ly}.self_attention.query_key_value\" for ly in range(25,29) ]\n",
    "\n",
    "peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, \n",
    "                         inference_mode=False, target_modules=lora_target_modules,\n",
    "                         r=4, lora_alpha=16, lora_dropout=0.1)\n",
    "model = get_peft_model(original_model,peft_config)\n",
    "print(model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='mean',ignore_index=tokenizer.pad_token_id)\n",
    "def generate(prompt,model_,tokenizer_):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    input_ids = input_ids.to(model_.device)\n",
    "    #model_.disable_adapter()\n",
    "    # 生成文本\n",
    "    inputs = {\"input_ids\":input_ids, \"max_length\":512, \n",
    "              \"num_beams\":5, \"no_repeat_ngram_size\":2,\n",
    "             \"top_k\":50, \"top_p\":0.95}\n",
    "    with torch.no_grad():\n",
    "        output = model_.generate(**inputs)\n",
    "    # 将生成的token解码成文本\n",
    "    generated_text = tokenizer_.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        {'params': [p for p in model.parameters() if p.requires_grad],'lr': 5e-5},\n",
    "    ]\n",
    ")\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(dataloader_random) ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f99738d75ed4dcc8f34bd40aa6be667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2997 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.6698, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6164, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tx/.conda/envs/llava/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:368: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      " the text is generated by AI\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "        read the following text, understand its style and words choices:\n",
      "         I would not support the idea of adding an extra hour and a half to the school day. While I understand that some students may want to participate in additional activities or stay after school to complete assignments and projects, I believe that the current seven-hour school day is already sufficient.\n",
      "\n",
      "I believe that parents may be concerned about their children's safety and well-being if they are away from home for an additional hour or two. Additionally, I think that students should have the opportunity to spend time with their families and engage in extracurricular activities outside of school.\n",
      "\n",
      "In my opinion, it is not fair for the students to decide if they want an extra hour or half an hour of school. The decision should be made by the parents and teachers, who are responsible for the students' education and well-being.\n",
      "\n",
      "I understand that students may have a lot of work to do at school, but I believe that it is important to maintain a balance between academic and personal responsibilities. Students should be encouraged to prioritize their studies, but they should also be given the opportunity to take breaks and engage in activities that bring them joy and fulfillment.\n",
      "\n",
      "In conclusion, while I understand the desire for an extra hour of school, I believe that the current seven-hour school day is sufficient. It is important to consider the safety and well-being of students, as well as the importance of maintaining a balance between academic and personal responsibilities..\n",
      "        is this text generated by AI?\n",
      "     the text is written by students\n",
      "tensor(0.0616, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "====================================================================================================\n",
      " the text is generated by AI\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "        read the following text, understand its style and words choices:\n",
      "        Hey, it's me, an average 8th grade student!  I'm super excited to write this essay about career prospects in technology and how it can affect my future.  I mean, it's not like I'm trying to impress my teacher or anything \n",
      "\n",
      "So, let's get started! \n",
      "\n",
      "Researching career prospects in technology, I found out that there are TONS of opportunities out there!  Like, did you know that by 2025, the technology industry is expected to create over 130,000 new jobs?!  That's like, a lot of jobs!  But, there are also some disadvantages. Like, it can be really competitive and it's hard to keep up with the latest tech trends. \n",
      "\n",
      "Now, let's talk about my personal interests.  I love coding and creating apps!  I'm also really into robotics and AI.  I think it's so cool how technology can be used to make things better and more efficient.  But, I'm not sure if I want to do it as a career... \n",
      "\n",
      "Deciding on a career at a young age can be tough.  Like, I don't want to pick something and then realize later that I hate it.  But, on the other hand, it's good to have a plan and know what you want to do with your life. \n",
      "\n",
      "Oh, and have you heard about the STEM programs in high school?!  They're like, super helpful in deciding what you want to do with your life!  You can learn about different careers and get hands-on experience. \n",
      "\n",
      "Finally, focusing more on technology can have BIG implications on the future of the United States!  Like, think about it... with all the new.\n",
      "        is this text generated by AI?\n",
      "     the text is written by students\n",
      "tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "====================================================================================================\n",
      "the text is written by students\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "        read the following text, understand its style and words choices:\n",
      "        Have you ever gave a person advice to make good choices?\n",
      "\n",
      "when you give people adivce they think more about the situation.\n",
      "\n",
      "3 good opinions to tell people that is make bad choices is make your mom proud, play sports, and stay away from negative energy.\n",
      "\n",
      "The first good opinion is making your mom proud. and what I mean by that is focusing on your school work and not getting in trouble inside and outside of school. There are a alot of people that is smart but just hang with the wrong crowd and that crowd get them in trouble.\n",
      "\n",
      "The second good opinion is playing sport. When playing sports it keep you out the streets and the wrong. It also keep keep you out of trouble and it leads you up a good path in life. When you are good at the sport you play you can get a scolarship but you can not mess it up for your self because you want to hang with your friends.\n",
      "\n",
      "The last good opinions staying away from negative energy.\n",
      "\n",
      "When you do not stay away from negative energy it can get you killed or in jail. When you stay away from the negativity you do not have to worry about people trying to harm you. Focous on school and get As and Bs but even if the negativity is in your school just walk away or say \" I dont feel like being bothered\". Then you will not be around that negative energy no more.\n",
      "\n",
      "Just listen to your family and friends when they are trying to help you be great in life. What they are telling you is good things that can keep you safe..\n",
      "        is this text generated by AI?\n",
      "    the text is written by students\n",
      "tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "====================================================================================================\n",
      "the text is written by students\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "        read the following text, understand its style and words choices:\n",
      "        Dear Principal,\n",
      "\n",
      "I support changing the policy. students should not be able join a particular sport team until they have reached a B average GPA. However, if they fall below a B average, they will not be able to participate in competitions, nevertheless practices are still mandatory. Having academic probation as a consequence encourages students to focus on their studies and participate in an extracurricular activity.\n",
      "\n",
      "Even though many students currently have a C average, participating in sports could be deleterious to a students performance in school if they are not required to hold a certain GPA. Many high schools have academic probation as a consequence when participating in school sports teams, and the students use this as motivation to work harder in school. If our school changed our policy so that students would need to maintain a B average GPA, many students would become more focused on their grades.\n",
      "\n",
      "On top of having the students more motivated to work hard in school, this policy would teach students that participating in an extracurricular is a privilege and if you abuse that privilege, there are consequences. If the students decide to only focus on sports and see it as a way to avoid there studies, there should be consequences. joining a sport should not be a frivolous activity, it should be viewed as a privilege. If you excel in school, you are then allowed to join a sporting team.\n",
      "\n",
      "I believe that altering the policy will be beneficial to our school. if students are required to hold at least a B average GPA, students will be more dedicated on their schooling. they will also view joining a sporting team as a privilege. I extremely support changing our school policy.\n",
      "\n",
      "Sincerely,\n",
      "\n",
      "Your pupil.\n",
      "        is this text generated by AI?\n",
      "    the text is written by students\n",
      "tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "====================================================================================================\n",
      "the text is written by students\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "        read the following text, understand its style and words choices:\n",
      "        Dear Principal,\n",
      "\n",
      "I understand that you are considering only letting students with a grade B average join the sports teams. I am sorry but I think this is not a good idea. Many students with a grade C average are just as smart as the other students and also as good at sports. Also, if you stop letting those students be on the teams you are not going to have enough people trying out to make a team.\n",
      "\n",
      "I am a grade C average student and I am on all the teams. I still keep my grades up. I have been a grade C student my whole life. That's just who I am, and if you are going to punish me for being the same student I have always been, I'm sorry, but I think you are not considering the students. We all try to be are best, sometimes we just fall short of what we are expected to do. You should look at this in the perspective of the students. We all love sports and love playing them. I hope you read this letter and consider every possibility.\n",
      "\n",
      "Sincerely,\n",
      "\n",
      "The Student Body..\n",
      "        is this text generated by AI?\n",
      "    the text is written by students\n",
      "tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "====================================================================================================\n",
      " the text is generated by AI\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "        read the following text, understand its style and words choices:\n",
      "         When faced with a problem, do you give up or do you put forth your best effort? Problems are a common occurrence in life, and how we respond to them is what truly defines us. An example of a problem is when you have a scholarship to a prestigious university, but your family needs you to stay close and help them. In such a situation, you must weigh the pros and cons and make a decision that is best for your future. It may be difficult to balance your education with your family's needs, but hard times bring out the best in us, and we must try our hardest to find a solution.\n",
      "\n",
      "People have different strategies when dealing with problems, but they all share one commonality: they do what they believe is best for themselves. They may try to see if it will help them or what they can use to their advantage to improve their situation. It's important to think things through before making a decision, as hasty choices can lead to unfavorable outcomes. Sometimes, problems may seem impossible to solve, but with perseverance and determination, we can find a way to overcome them.\n",
      "\n",
      "Problems can affect not only ourselves but also others, such as friends, family, or strangers. For example, if you have a scholarship to a university far away, but your family doesn't want you to leave, you must consider the impact of your decision on both yourself and your family. It may be a difficult choice, but ultimately, it's important to do what is best for your future.\n",
      "\n",
      "It's important to remember that we are never alone when faced with a problem. There are people who can help us, whether it's a friend, family member, or mentor. Even if we try our best, there may be times when we need the help of others. It's okay to ask for help and to lean on others when we need support.\n",
      "\n",
      "Problems can range in size and complexity, but they are all problems that we can't prevent from happening. It's important to approach them with a positive attitude and to do our best to find a solution. We can learn from our mistakes and use them to improve our lives. As Duke Ellington once said, \"A problem is a chance for you to do your best.\"\n",
      "\n",
      "In conclusion, when faced with a problem, it's important to put forth our best.\n",
      "        is this text generated by AI?\n",
      "     the text is written by students\n",
      "tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "====================================================================================================\n",
      " the text is generated by AI\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "        read the following text, understand its style and words choices:\n",
      "        Productivity versus Idleness: A Comparative Analysis\n",
      "\n",
      "Introduction\n",
      "\n",
      "Productivity and idleness are two contrasting ways of life that can have significant impacts on a person's life and career opportunities. While productivity is often associated with success and recognition, idleness can lead to a lack of fulfillment and missed opportunities. In this essay, I will explore the benefits and drawbacks of both productivity and idleness, examine examples of successful people who have worked hard to gain recognition, and analyze the effects of idleness on a person's life and career opportunities.\n",
      "\n",
      "Benefits of Productivity\n",
      "\n",
      "Productivity is the state of being highly efficient and productive in one's work. It is often associated with success and recognition, as it allows individuals to accomplish more in less time. Productivity can lead to increased job satisfaction, higher salaries, and better career opportunities. For example, individuals who are productive and efficient in their work are often seen as valuable assets to their employers and are more likely to be promoted or given additional responsibilities.\n",
      "\n",
      "Drawbacks of Idleness\n",
      "\n",
      "Idleness, on the other hand, is the state of being unproductive and inactive. It can lead to a lack of fulfillment and missed opportunities. Idleness can also have negative impacts on a person's mental and physical health. For example, individuals who spend too much time sitting and not engaging in physical activity are at an increased risk of obesity, heart disease, and other health issues. Additionally, idleness can lead to feelings of boredom and dissatisfaction, which can negatively impact a person's overall well-being.\n",
      "\n",
      "Examples of Successful People\n",
      "\n",
      "There are many successful people who have worked hard to gain recognition. For example, Oprah Winfrey, one of the most successful media personalities of all time, rose from a poverty-stricken childhood to become a household name. Her success was due in large part to her hard work, determination, and productivity. Similarly, Elon Musk, the CEO of Tesla and SpaceX, has achieved incredible success through his hard work and dedication to his projects. Both Winfrey and Musk are examples of individuals who have worked hard to achieve success and recognition.\n",
      "\n",
      "Effects of Idleness\n",
      "\n",
      "Idleness can have significant negative effects on a person's life and career opportunities..\n",
      "        is this text generated by AI?\n",
      "     the text is written by students\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "====================================================================================================\n",
      "the text is written by students\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "        read the following text, understand its style and words choices:\n",
      "        Do you seek multiple opinions when making a hard choice or decision? I feel it is important to discuss your options with several people before you stick to a decision. There are many times when I have made the wrong decision because I only talked to one person about my choice. Including when I joined the softball team after talking to only my mom. She thought that I would have a great time and learn a lot, but soon after practices started I learned that this was not the sport for me. Now, whenever I have a decision to make, big or small, I always take advice from more than one person.\n",
      "\n",
      "When I ask for advice I enjoy talking to multiple people to get their perspective on my options. One example of this is when you are applying to colleges. One individual may tell you that a certain college was wonderful and that they had a great educational experience there, while another person may tell you that they hated that specific college and didn't learn anything while at it. Other examples include trying out for a new sport, taking music lessons, or how to study for an upcoming test. I think it is important to get and give useful advice.\n",
      "\n",
      "When making a choice you don't always consider all of your options or alternatives, asking someone for their advice will help you when its time to make the decision. It will give you insight on how your choice will affect you and others around you. While it may be easy to get advice you also need to know how to give the best advice to someone in need of it. Your friends, family, teachers, and peers will give you advice, and get advice from you all throughout your life.\n",
      "\n",
      "All people give you different advice due to their experience, opinions, education, and ability. I learned that when trying to make the best choice it is important to seek multiple opinions from a number of people. If you don't get advice from more than one person it may result in not picking the best choice or decision. When making a choice be sure to analyze all the advice given to you and take every aspect into consideration. Always seeking multiple opinions will help you make the best choices and decisions..\n",
      "        is this text generated by AI?\n",
      "    the text is written by students\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    num_epochs = 1\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        pbar = tqdm(dataloader_random)\n",
    "        for step, batch_a in enumerate(pbar):\n",
    "            batch,_,final_text,label_text = batch_a\n",
    "            batch = {k: v.cuda() for k, v in batch.items()}\n",
    "            #print(batch)\n",
    "            #outputs = model(batch['input_ids'],labels=batch['labels'])\n",
    "            labels_tensor = batch['labels']\n",
    "            outputs = model(batch['input_ids'])\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            logits = logits[...,:-1,:].contiguous()\n",
    "            labels_tensor = labels_tensor[...,1:].contiguous()\n",
    "            \n",
    "            \n",
    "            #loss = outputs.loss\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels_tensor.view(-1))\n",
    "        \n",
    "            total_loss += loss.detach().float()\n",
    "            #print(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            if(step % 100 == 0):\n",
    "                pbar.set_description(f\"step {step} loss {loss}\")\n",
    "                print(loss)\n",
    "            if(step  % 200 == 0 and step > 0):\n",
    "                from IPython.display import display,clear_output,update_display\n",
    "                test = generate(final_text[0],model,tokenizer)\n",
    "                print(\"=\"* 100)\n",
    "                print(label_text[0])\n",
    "                print(\">\"* 100)\n",
    "                print(test)\n",
    "                \n",
    "\n",
    "                \n",
    "            \n",
    "train()           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "import time\n",
    "clear_output()\n",
    "\n",
    "out = widgets.Output()\n",
    "\n",
    "for fruit in [1,2,3]:\n",
    "    time.sleep(1)\n",
    "    clear_output()\n",
    "    #out.append_stdout(f\"Do you like {fruit}s?\")\n",
    "    print(fruit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_trained = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tx/.conda/envs/llava/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:368: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    read the following text, understand its style and words choices:\\n    maybe its is not rigiht what.\\n    is this text generated by AI?\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"\"\"\n",
    "    read the following text, understand its style and words choices:\n",
    "    maybe its is not rigiht what.\n",
    "    is this text generated by AI?\n",
    "\"\"\",model_trained,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data):\n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = \"./data/test_essays.csv\"\n",
    "test_data = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000aaaa</td>\n",
       "      <td>2</td>\n",
       "      <td>Aaa bbb ccc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1111bbbb</td>\n",
       "      <td>3</td>\n",
       "      <td>Bbb ccc ddd.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2222cccc</td>\n",
       "      <td>4</td>\n",
       "      <td>CCC ddd eee.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  prompt_id          text\n",
       "0  0000aaaa          2  Aaa bbb ccc.\n",
       "1  1111bbbb          3  Bbb ccc ddd.\n",
       "2  2222cccc          4  CCC ddd eee."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "save_dir =  \"/home/tx/workspace/saved_model\"\n",
    "if(not os.path.exists(save_dir)):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_name = os.path.join(save_dir,'bloom_3b_AI_detector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(model_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
