{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_proxy():\n",
    "    import os\n",
    "    cache_dir = \"/home/tx/workspace/cache\"  # 替换为你想要的缓存目录的路径\n",
    "    os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "    # 代理\n",
    "    os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "    os.environ['https_proxy'] = 'http://127.0.0.1:7890'\n",
    "    os.environ['no_proxy'] = '127.0.0.1,localhost'\n",
    "    os.environ['HTTP_PROXY'] = 'http://127.0.0.1:7890'\n",
    "    os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'\n",
    "    os.environ['NO_PROXY'] = '127.0.0.1,localhost'\n",
    "    return\n",
    "set_proxy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cache_dir = \"/home/tx/workspace/cache\"  # 替换为你想要的缓存目录的路径\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "from transformers import AutoConfig, LlamaConfig \n",
    "cache_dir = \"/home/tx/workspace/cache\" \n",
    "model_name = \"bigscience/bloom-3b\" # bigscience/bloom-3b\n",
    "#model_name = \"ChocoWu/nextgpt_7b_tiva_v0\"#\"liuhaotian/llava-v1.5-7b\"\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_name,trust_remote_code=True, cache_dir=cache_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "\n",
    "peft_parameters_path = \"./ai_detector_peft/\"\n",
    "#original_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "For effortless bug reporting copy-paste your error into this form: https://docs.google.com/forms/d/e/1FAIpQLScPB8emS3Thkp66nvqwmjTEgxp8Y9ufuWTzFyr9kJ5AoI47dQ/viewform?usp=sf_link\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.7/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 5.2\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/tx/.conda/envs/llava/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tx/.conda/envs/llava/lib/python3.11/site-packages/bitsandbytes/cuda_setup/paths.py:93: UserWarning: /home/tx/.conda/envs/llava did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(original_model,model_id=\"./ai_detector_peft/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='mean',ignore_index=tokenizer.pad_token_id)\n",
    "def generate(prompt,model_,tokenizer_):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    input_ids = input_ids.to(model_.device)\n",
    "    #model_.disable_adapter()\n",
    "    # 生成文本\n",
    "    inputs = {\"input_ids\":input_ids, \"max_length\":512, \n",
    "              \"num_beams\":5, \"no_repeat_ngram_size\":2,\n",
    "             \"top_k\":50, \"top_p\":0.95}\n",
    "    with torch.no_grad():\n",
    "        output = model_.generate(**inputs)\n",
    "    # 将生成的token解码成文本\n",
    "    generated_text = tokenizer_.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(prompt,feature_text):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    text=  f\"\"\"\n",
    "        read the following text, understand its style and words choices:\n",
    "        {feature_text}.\n",
    "        is this text generated by AI?\n",
    "    \"\"\"\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tx/.conda/envs/llava/lib/python3.11/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 623, but `max_length` is set to 512. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (512) must match the existing size (624) at non-singleton dimension 0.  Target sizes: [512].  Tensor sizes: [624]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/tx/workspace/code_for_compete/AI_GENERATED_DEECTER/ai_detector_pred.ipynb 单元格 6\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B2.tcp.vip.cpolar.cn/home/tx/workspace/code_for_compete/AI_GENERATED_DEECTER/ai_detector_pred.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B2.tcp.vip.cpolar.cn/home/tx/workspace/code_for_compete/AI_GENERATED_DEECTER/ai_detector_pred.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m final_text \u001b[39m=\u001b[39m generate_prompt(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mthis is not what I want\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B2.tcp.vip.cpolar.cn/home/tx/workspace/code_for_compete/AI_GENERATED_DEECTER/ai_detector_pred.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m generate(final_text,model_\u001b[39m=\u001b[39;49mmodel,tokenizer_\u001b[39m=\u001b[39;49mtokenizer)\n",
      "\u001b[1;32m/home/tx/workspace/code_for_compete/AI_GENERATED_DEECTER/ai_detector_pred.ipynb 单元格 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B2.tcp.vip.cpolar.cn/home/tx/workspace/code_for_compete/AI_GENERATED_DEECTER/ai_detector_pred.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m inputs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m:input_ids, \u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m512\u001b[39m, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B2.tcp.vip.cpolar.cn/home/tx/workspace/code_for_compete/AI_GENERATED_DEECTER/ai_detector_pred.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnum_beams\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m5\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mno_repeat_ngram_size\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m2\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B2.tcp.vip.cpolar.cn/home/tx/workspace/code_for_compete/AI_GENERATED_DEECTER/ai_detector_pred.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m          \u001b[39m\"\u001b[39m\u001b[39mtop_k\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m50\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtop_p\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m0.95\u001b[39m}\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B2.tcp.vip.cpolar.cn/home/tx/workspace/code_for_compete/AI_GENERATED_DEECTER/ai_detector_pred.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B2.tcp.vip.cpolar.cn/home/tx/workspace/code_for_compete/AI_GENERATED_DEECTER/ai_detector_pred.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     output \u001b[39m=\u001b[39m model_\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B2.tcp.vip.cpolar.cn/home/tx/workspace/code_for_compete/AI_GENERATED_DEECTER/ai_detector_pred.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# 将生成的token解码成文本\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B2.tcp.vip.cpolar.cn/home/tx/workspace/code_for_compete/AI_GENERATED_DEECTER/ai_detector_pred.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m generated_text \u001b[39m=\u001b[39m tokenizer_\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/llava/lib/python3.11/site-packages/peft/peft_model.py:975\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    973\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model\u001b[39m.\u001b[39mgeneration_config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeneration_config\n\u001b[1;32m    974\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 975\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    976\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model\u001b[39m.\u001b[39mprepare_inputs_for_generation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model_prepare_inputs_for_generation\n",
      "File \u001b[0;32m~/.conda/envs/llava/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/llava/lib/python3.11/site-packages/transformers/generation/utils.py:1685\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1678\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1679\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1680\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   1681\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1682\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1683\u001b[0m     )\n\u001b[1;32m   1684\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1685\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_search(\n\u001b[1;32m   1686\u001b[0m         input_ids,\n\u001b[1;32m   1687\u001b[0m         beam_scorer,\n\u001b[1;32m   1688\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1689\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1690\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1691\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1692\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1693\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1694\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1695\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1696\u001b[0m     )\n\u001b[1;32m   1698\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SAMPLE:\n\u001b[1;32m   1699\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m~/.conda/envs/llava/lib/python3.11/site-packages/transformers/generation/utils.py:3111\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3108\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3109\u001b[0m             this_peer_finished \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m-> 3111\u001b[0m sequence_outputs \u001b[39m=\u001b[39m beam_scorer\u001b[39m.\u001b[39;49mfinalize(\n\u001b[1;32m   3112\u001b[0m     input_ids,\n\u001b[1;32m   3113\u001b[0m     beam_scores,\n\u001b[1;32m   3114\u001b[0m     next_tokens,\n\u001b[1;32m   3115\u001b[0m     next_indices,\n\u001b[1;32m   3116\u001b[0m     pad_token_id\u001b[39m=\u001b[39;49mpad_token_id,\n\u001b[1;32m   3117\u001b[0m     eos_token_id\u001b[39m=\u001b[39;49meos_token_id,\n\u001b[1;32m   3118\u001b[0m     max_length\u001b[39m=\u001b[39;49mstopping_criteria\u001b[39m.\u001b[39;49mmax_length,\n\u001b[1;32m   3119\u001b[0m     beam_indices\u001b[39m=\u001b[39;49mbeam_indices,\n\u001b[1;32m   3120\u001b[0m )\n\u001b[1;32m   3122\u001b[0m \u001b[39mif\u001b[39;00m return_dict_in_generate:\n\u001b[1;32m   3123\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m output_scores:\n",
      "File \u001b[0;32m~/.conda/envs/llava/lib/python3.11/site-packages/transformers/generation/beam_search.py:392\u001b[0m, in \u001b[0;36mBeamSearchScorer.finalize\u001b[0;34m(self, input_ids, final_beam_scores, final_beam_tokens, final_beam_indices, max_length, pad_token_id, eos_token_id, beam_indices)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[39m# fill with hypotheses and eos_token_id if the latter fits in\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[39mfor\u001b[39;00m i, (hypo, best_idx) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(best, best_indices)):\n\u001b[0;32m--> 392\u001b[0m     decoded[i, : sent_lengths[i]] \u001b[39m=\u001b[39m hypo\n\u001b[1;32m    394\u001b[0m     \u001b[39mif\u001b[39;00m indices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    395\u001b[0m         indices[i, : \u001b[39mlen\u001b[39m(best_idx)] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(best_idx)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (512) must match the existing size (624) at non-singleton dimension 0.  Target sizes: [512].  Tensor sizes: [624]"
     ]
    }
   ],
   "source": [
    "model = model.cuda()\n",
    "final_text = generate_prompt(\"\",\"this is not what I want\"*100)\n",
    "generate(final_text,model_=model,tokenizer_=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
